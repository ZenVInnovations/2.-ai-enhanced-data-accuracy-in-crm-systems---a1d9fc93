# -*- coding: utf-8 -*-
"""Customer Segmentation and Churn Analysis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wiwyrBFrvqWueSqY__8HLEjYXnAGmVkL

IMPORTS
"""

import pandas as pd
import plotly.graph_objs as go
import plotly.offline as pyoff
from sklearn.cluster import KMeans
from matplotlib import pyplot as plt
import seaborn as sns

"""DISPLAYING DATASET"""

df = pd.read_csv('/content/drive/MyDrive/college/sem7/FYP/docs/dataset/cell2celltrain.csv')
df.head()

"""CUSTOMER SEGMENTATION"""

df_segmentation = df
df_segmentation

customer_per_MonthsInService = df_segmentation
customer_per_MonthsInService

plot_data = [
    go.Histogram(
        x=customer_per_MonthsInService['MonthsInService']
    )
]

plot_layout = go.Layout(
        title='MonthsInService'
    )
fig = go.Figure(data=plot_data, layout=plot_layout)
pyoff.iplot(fig)

sse = {}
for k in range(1, 11):
    kmeans = KMeans(n_clusters=k, max_iter=1000, random_state=42).fit(customer_per_MonthsInService[['CustomerID', 'MonthsInService']])
    customer_per_MonthsInService["MonthsInServiceCluster"] = kmeans.labels_
    sse[k] = kmeans.inertia_ # SSE to closest cluster centroid
plt.title('The Elbow Method')
plt.xlabel('Number of clusters')
plt.ylabel('SSE')
sns.pointplot(list(sse.keys()), list(sse.values()))
plt.show()

kmeans = KMeans(n_clusters=3)
kmeans.fit(customer_per_MonthsInService[['MonthsInService']])
customer_per_MonthsInService['MonthsInServiceCluster'] = kmeans.predict(customer_per_MonthsInService[['MonthsInService']])

def order_cluster(cluster_field_name, target_field_name,df,ascending):
    new_cluster_field_name = 'new_' + cluster_field_name
    df_new = df.groupby(cluster_field_name)[target_field_name].mean().reset_index()
    df_new = df_new.sort_values(by=target_field_name,ascending=ascending).reset_index(drop=True)
    df_new['index'] = df_new.index
    df_final = pd.merge(df,df_new[[cluster_field_name,'index']], on=cluster_field_name)
    df_final = df_final.drop([cluster_field_name],axis=1)
    df_final = df_final.rename(columns={"index":cluster_field_name})
    return df_final

df_segmentation_MonthsInService = order_cluster('MonthsInServiceCluster', 'MonthsInService', customer_per_MonthsInService,True)
df_segmentation_MonthsInService.groupby('MonthsInServiceCluster')['MonthsInService'].describe()

df_segmentation_MonthsInService

df_churn_prone_customers = df_segmentation_MonthsInService[df_segmentation_MonthsInService['MonthsInServiceCluster'] == 0]

# 1st cluster: prone to churningd
df_churn_prone_customers

dataset = df_churn_prone_customers #data set shape

# dataset shape
dataset.shape

# missing values
dataset.isnull().values.any()

# churn
dataset['Churn'].value_counts()

# missing values in churn column
dataset['Churn'].isna().sum()

# removing features that don't seem to have a lot of effect on the model
dataset = dataset.drop(['HandsetPrice', 'HandsetModels', 'Homeownership', 'MaritalStatus', 'UniqueSubs', 'PeakCallsInOut','OffPeakCallsInOut','DroppedBlockedCalls',
             'RetentionCalls','InboundCalls','OverageMinutes','ReceivedCalls', 'OwnsMotorcycle', 'NonUSTravel', 'OwnsComputer', 'RVOwner', 'TruckOwner', 'HandsetRefurbished',
             'HandsetWebCapable', 'Handsets'], axis=1)

dataset

"""PRE-PROCESSING DATASET"""

# Pre-processing (Normalizing values)

binary_cols=[]
multi_Value=[]
for col in dataset.columns :
    if dataset[col].dtype =='object':
        if dataset[col].unique().shape[0]==2:
            binary_cols.append(col)
        else:
            multi_Value.append(col)

binary_cols

multi_Value
# using label encoder for normalizing multi value columns
# using label encoder for normalizing multi value columns
from sklearn.preprocessing import LabelEncoder
LE_cat = LabelEncoder()
for i in multi_Value:
    dataset[i] = LE_cat.fit_transform(dataset[i].astype(str))

dataset

dataset.shape

# using get dummies for normalizing binary value columns
Binary_cols_except_churn=binary_cols[1:]
dataset_yes_no=dataset[Binary_cols_except_churn]
dataset_yes_no

dfDummies = pd.get_dummies(dataset_yes_no, prefix = dataset_yes_no.columns)
dfDummies.shape

dataset.shape

removed_binary_cols=dataset.drop(binary_cols,axis=1)
removed_binary_cols

clean_dataframe=pd.concat([removed_binary_cols, dfDummies, dataset['Churn']], axis=1)
clean_dataframe['Churn'].replace('Yes', 1, inplace=True)
clean_dataframe['Churn'].replace('No', 0, inplace=True)

clean_dataframe

# Imputation preserves all cases by replacing missing data with an estimated value based on other available information.

from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=5)
final_dataset = pd.DataFrame(imputer.fit_transform(clean_dataframe),columns = clean_dataframe.columns)

bar_df=final_dataset.isna().sum()
bar_df[bar_df>0]

"""CUSTOMER CHURN"""

# XG Boost

from sklearn.model_selection import train_test_split
y = final_dataset["Churn"]
X = final_dataset.drop(["Churn", 'CustomerID'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20, random_state=42)

from xgboost.sklearn import XGBClassifier
xgb_model = XGBClassifier().fit(X_train, y_train)
y_pred = xgb_model.predict(X_test)

xgb_model

model = xgb_model.fit(X_train, y_train)
y_pred = model.predict(X_test)

from sklearn.metrics import classification_report
print(classification_report(y_test,y_pred))

# Decision tree

y = final_dataset["Churn"]
X = final_dataset.drop(["Churn", 'CustomerID'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20, random_state=42)

from sklearn.tree import DecisionTreeClassifier
DT = DecisionTreeClassifier(criterion='entropy', random_state=1)
DT.fit(X_train, y_train)

prediction = DT.predict(X_test)

print(classification_report(y_test,prediction))

# Stochastic gradient descent

y = final_dataset["Churn"]
X = final_dataset.drop(["Churn", 'CustomerID'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20, random_state=42)

from sklearn.linear_model import SGDClassifier
SGDC = SGDClassifier()
SGDC.fit(X_train, y_train)

predictions = SGDC.predict(X_test)

print(classification_report(y_test,predictions))

# Logistic regression

y = final_dataset["Churn"]
X = final_dataset.drop(["Churn", 'CustomerID'], axis=1)
X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.20, random_state=42)

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, accuracy_score
classifier = LogisticRegression(max_iter=4000)
classifier.fit(X_train, y_train)

predictions = classifier.predict(X_test)

print(classification_report(y_test,predictions))
